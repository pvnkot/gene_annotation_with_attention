{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Config for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sample_size = 500\n",
    "negative_sample_size = 500\n",
    "positive_test_sample_size = 100\n",
    "negative_test_sample_size = 100\n",
    "window_size = 3\n",
    "embedding_size = 5\n",
    "num_epochs = 1000\n",
    "batch_size = 50\n",
    "hidden_layer_size = 500\n",
    "hidden_layer_size_2 = 100\n",
    "with_attention = False\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading first 1000 rows (500 positive, 500 negative) into data for basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = pd.read_csv('positive_sample.txt', header = None, nrows = positive_sample_size)\n",
    "positive_data.columns = [\"Gene\"]\n",
    "negative_data = pd.read_csv('negative_sample.txt', header = None, nrows = negative_sample_size)\n",
    "negative_data.columns = [\"Gene\"]\n",
    "positive_test_data = pd.read_fwf('positive_sample_test.txt', header = None)\n",
    "positive_test_data.columns = [\"Gene\"]\n",
    "negative_test_data = pd.read_fwf('negative_sample_test.txt', header = None)\n",
    "negative_test_data.columns = [\"Gene\"]\n",
    "train_data = positive_data.append(negative_data)\n",
    "test_data = positive_test_data.append(negative_test_data)\n",
    "#print(data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper methods for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper methods\n",
    "def get_labels(positive_sample_size, negative_sample_size):\n",
    "    #labels = []\n",
    "    #positive_\n",
    "    labels = torch.cat((torch.ones([positive_sample_size, 1], dtype=torch.float), torch.zeros([negative_sample_size, 1], dtype=torch.float)))\n",
    "    #print(labels)\n",
    "    return labels\n",
    "def embeddings_helper(window_size):\n",
    "    vocab_set = set()\n",
    "    def generate_vocab_helper(set, k): \n",
    "        n = len(set)  \n",
    "        generate_vocab(set, \"\", n, k) \n",
    "    def generate_vocab(set, prefix, n, k): \n",
    "        if (k == 0) : \n",
    "            vocab_set.add(prefix)\n",
    "            return\n",
    "        for i in range(n): \n",
    "            newPrefix = prefix + set[i] \n",
    "            generate_vocab(set, newPrefix, n, k - 1) \n",
    "    def generate_embed_map(n):\n",
    "        alphabet = ['0','1','2','3','4']\n",
    "        generate_vocab_helper(alphabet, n)\n",
    "\n",
    "        vocab_set_1 = sorted(vocab_set)\n",
    "        vocab_map = {}\n",
    "\n",
    "        for i in range(len(vocab_set_1)):\n",
    "            vocab_map[vocab_set_1[i]] = i\n",
    "        return vocab_map\n",
    "    return generate_embed_map(window_size)\n",
    "\n",
    "def return_embeddings(vocabulary):\n",
    "    embeds = nn.Embedding(len(vocabulary), embedding_size)\n",
    "    embeddings = {}\n",
    "    for word in vocabulary:\n",
    "        embeddings[word] = embeds(torch.tensor(vocabulary[word], dtype=torch.long))\n",
    "    return embeddings\n",
    "\n",
    "#ATG, GTG, TTG\n",
    "def is_start_codon(codon):\n",
    "    start_codons = ['143', '343', '443']#['ATG', 'GTG', 'TTG']\n",
    "    if codon in start_codons:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def set_hyperparameters(win_size, epochs, batchsize):\n",
    "    window_size = win_size\n",
    "    num_epochs = epochs\n",
    "    batch_size = batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating word indexes for permutations of protein bases (words of vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = embeddings_helper(window_size)\n",
    "embeddings = return_embeddings(vocabulary)\n",
    "#print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training data with training inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_and_labels(with_attention, datapoints, positive_sample_size, negative_sample_size):\n",
    "    list_of_tensors = []\n",
    "    #\n",
    "    # In an array, place 1s for codons that match the start pattern and 0s for codons that do not.\n",
    "    #\n",
    "    codon_arr = []\n",
    "    for data in datapoints.itertuples():\n",
    "        gene = data.Gene\n",
    "\n",
    "        for i in range(len(gene) - window_size + 1):\n",
    "            codon = gene[i:i+window_size]\n",
    "            weight = 1\n",
    "            if with_attention and is_start_codon(codon):\n",
    "                weight = i+1\n",
    "            if i == 0:\n",
    "                first_tensor = embeddings[gene[i:i+window_size]] * weight\n",
    "            else:\n",
    "                first_tensor = torch.cat((first_tensor, weight * embeddings[gene[i:i+window_size]]), 0)\n",
    "\n",
    "        list_of_tensors.append(first_tensor)\n",
    "    inputs = torch.stack(list_of_tensors)\n",
    "    labels = get_labels(positive_sample_size, negative_sample_size)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementing the FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size*(len(positive_data.Gene[0]) - window_size + 1), hidden_layer_size)\n",
    "        self.relu = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, hidden_layer_size_2)\n",
    "        #self.prelu = nn.Sigmoid()\n",
    "        self.out = nn.Linear(hidden_layer_size_2, 1)\n",
    "        #self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.fc1(x)\n",
    "        h1 = self.relu(a1)\n",
    "        #dout = self.dout(h1)\n",
    "        a2 = self.fc2(h1)\n",
    "        h2 = self.relu(a2)\n",
    "        a3 = self.out(h2)\n",
    "        y = self.relu(a3)\n",
    "        return y\n",
    "    \n",
    "def train_epoch(model, inputs, labels, optimizer, criterion, batch_size):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in range(0, inputs.size(0), batch_size):\n",
    "        data_batch = inputs[i:i + batch_size, :]\n",
    "        labels_batch = labels[i:i + batch_size, :]\n",
    "        data_batch = autograd.Variable(data_batch)\n",
    "        labels_batch = autograd.Variable(labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # (1) Forward\n",
    "        labels_hat = net(data_batch)\n",
    "        #print(labels_hat)\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(labels_hat, labels_batch)\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        optimizer.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    loss = sum(losses)/len(losses)\n",
    "    return loss\n",
    "    \n",
    "def train_model(model, inputs, labels, optimizer, criterion, with_attention, batch_size):\n",
    "    losses = []\n",
    "    print('Training the model:')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        losses.append(train_epoch(model, inputs, labels, optimizer, criterion, batch_size))\n",
    "#         if epoch % 25 == 0:    #print every 25 mini-batches\n",
    "#             print('[%d, %5d] loss: %.9f' %\n",
    "#                 (epoch + 1, epoch + 1, running_loss/len(inputs)))\n",
    "#             #running_loss = 0.0\n",
    "    model_name = 'fc_with_attention_4k_e_1k.pt' if with_attention else 'fc_4k_e_1k.pt'\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print('Finished. Training took %.3f' %((time.time() - start_time)/60), 'minutes.')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network\n",
    "def training():\n",
    "    net = Net()\n",
    "    inputs, labels = get_inputs_and_labels(with_attention, train_data, positive_sample_size, negative_sample_size)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)\n",
    "    losses = train_model(net, inputs, labels, optimizer, criterion, with_attention, batch_size)\n",
    "    if losses != None:\n",
    "        plt.plot(losses)\n",
    "        title = 'Loss vs Epochs for: ' + (str)(positive_sample_size+negative_sample_size) + ' data points and ' + (str)(num_epochs) + ' epochs'\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = Net()\n",
    "#training()\n",
    "#training(net, training_inputs, training_labels, optimizer, criterion, with_attention, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Window size of 3, 1000 epochs and batch size of 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model:\n",
      "Finished. Training took 0.330 minutes.\n",
      "Accuracy is:  49.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_hyperparameters(3, 1000, 50)\n",
    "training()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Window size of 3, 2000 epochs and batch size of 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  55.00000000000001\n"
     ]
    }
   ],
   "source": [
    "set_hyperparameters(3, 2000, 50)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
